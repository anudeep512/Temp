Current focused Time Entites:

A) Start Date
   End Date

B) Date period
   - today
   - yesterday
   - 2 days ago
   - 1 week ago
   - June 6th 2024
   - day before yesterday

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Q: What was the total notional traded on May 13 for GSET in apac?
P: DateFrom: 2024-05-13, DateTo: 2024-05-13, BusinessLine: GSET, region: apac

Q: What was the total notional volume traded on May 13 for GSET in apac?
P: DateFrom: 2024-05-13, DateTo: 2024-05-13, BusinessLine: GSET, region: apac

Q: What was the total notional volume traded from May 13 until june 16th for GSET in apac?
P: DateFrom: 2024-05-13, DateTo: 2024-06-16, BusinessLine: GSET, region: apac

Q: Could you tell me how much was traded in total notional value at NASD by BTIG on May 13th in americas?
P: DateFrom: 2024-05-13, DateTo: 2024-05-13, ExecutionPoint: NASD, Tuid: BTIG, Region: americas

Q: In emea Can you find out what the total notional traded was on May 13th at NASD by BTIG?
P: DateFrom: 2024-05-13, DateTo: 2024-05-13, ExecutionPoint: NASD, Tuid: BTIG, Region: emea

Q: Provide me the information about the total notional traded on May 13 on NASD by BTIG in emea?
P: DateFrom: 2024-05-13, DateTo: 2024-05-13, ExecutionPoint: NASD, Tuid: BTIG, Region: emea

Q: What was the total notional traded in the AMERICAS for SINGLE_SHARES on May 15th, 2024?
P: DateFrom: 2024-05-15, DateTo: 2024-05-15, BusinessLine: SINGLE_SHARES, Region: AMERICAS

Q: Do you know how much notional was traded in single shares across the Americas on May 15th, 2024?
P: DateFrom: 2024-05-15, DateTo: 2024-05-15, BusinessLine: SINGLE_SHARES, Region: AMERICAS

Q: I'm curious, how much notional was traded in single shares in the Americas on May 15th, 2024?
P: DateFrom: 2024-05-15, DateTo: 2024-05-15, BusinessLine: SINGLE_SHARES, Region: AMERICAS

Q: Can you provide the total notional amount for single shares traded in the Americas on May 15th, 2024?
P: DateFrom: 2024-05-15, DateTo: 2024-05-15, BusinessLine: SINGLE_SHARES, Region: AMERICAS

Q: Do you happen to know the total notional traded for SINGLE_SHARES in the Americas on May 15th, 2024?
P: DateFrom: 2024-05-15, DateTo: 2024-05-15, BusinessLine: SINGLE_SHARES, Region: AMERICAS

Q: Could you find out how much notional was traded in SINGLE_SHARES in the Americas on May 15th, 2024?
P: DateFrom: 2024-05-15, DateTo: 2024-05-15, BusinessLine: SINGLE_SHARES, Region: AMERICAS

Q: I'm wondering about the notional value of single shares traded in the Americas on May 15th, 2024. Do you have that info?
P: DateFrom: 2024-05-15, DateTo: 2024-05-15, BusinessLine: SINGLE_SHARES, Region: AMERICAS

Q: Hey, do you know how much was traded in single shares in the Americas on May 15th, 2024?
P: DateFrom: 2024-05-15, DateTo: 2024-05-15, BusinessLine: SINGLE_SHARES, Region: AMERICAS

Q: How much was traded through the execution point WFAQ by the client SINBCVVR Inc. on June 1st, 2024 in EMEA? (DOUBT NO MENTION OF NOTIONAL)
P: DateFrom: 2024-06-01, DateTo: 2024-06-01, ExecutionPoint: WFAQ, TuID: 5ZGM, region: EMEA

Q: What was the trading volume for RLVLCMKGX Inc. in EMEA using their StarID on April 20th, 2024? (DOUBT NO MENTION OF NOTIONAL)
P: DateFrom: 2024-04-20, DateTo: 2024-04-20, Region: EMEA, StarID: 488256

Q: What was the notional amount traded at the execution point WLDS for the business line GSET in APAC on March 5th, 2024?
P: DateFrom: DateFrom: 2024-03-05, DateTo: 2024-03-05, BusinessLine: GSET, ExecutionPoint: WLDS, Region: APAC

Q: How much was the total notional traded in the APAC region for all business lines on January 1st, 2024?
P: DateFrom: 2024-01-01, DateTo: 2024-01-01, Region: APAC

Q: What is the total notional traded value in EMEA for the client DYAIFGP Inc. using StarID on February 20th, 2024?
P: DateFrom: 2024-02-20, DateTo: 2024-02-20, Region: EMEA, StarID: 11207379

Q: Would you be able to tell me the total notional traded in EMEA for the client DYAIFGP Inc. with StarID on February 20th, 2024?
P: DateFrom: 2024-02-20, DateTo: 2024-02-20, Region: EMEA, StarID: 11207379

Q: Total notional traded in EMEA on February 20th, 2024?
P: DateFrom: 2024-02-20, DateTo: 2024-02-20, Region: EMEA

Q: Notion volume traded in APAC on February 20th, 2024?
P: DateFrom: 2024-02-20, DateTo: 2024-02-20, Region: EMEA

Q: Notion volume traded in APAC from Feb 5th, 2023 until Feb 16th 2024?
P: DateFrom: 2024-02-05, DateTo: 2024-02-16, Region: EMEA

Q: What was the notional traded amount for the business line SINGLE_SHARES in the AMERICAS region on October 21st, 2024?
P: DateFrom: 2024-10-21, DateTo: 2024-10-21, BusinessLine: SINGLE_SHARES, Region: AMERICAS

Q: How much notional value was traded by QTJFMQAT Inc. with StarID 11236780 at execution point QSTD in AMERICAS on August 3rd, 2024?
P: DateFrom: 2024-08-03, DateTo: 2024-08-03, ExecutionPoint: QSTD, StarID: 11236780, region: AMERICAS

Q: Can you provide the total notional traded on April 12th, 2024, by all traders using execution point WSPB in the EMEA region?
P: DateFrom: 2024-04-12, DateTo: 2024-04-12, ExecutionPoint: WSPB, Region: EMEA

Q: What was the notional amount traded on the business line GSET in EMEA yesterday?
P: DateFrom: yesterday, DateTo: yesterday, BusinessLine: GSET, Region: EMEA

Q: How much notional value was traded through execution point WFAQ 1 week ago in AMERICAS Region?
P: DateFrom: 1 week ago, DateTo: 1 week ago, ExecutionPoint: WFAQ, Region: AMERICAS

Q: What is the notional amount traded by YQDZV Inc. in APAC Region with TuID 1PQVARRA 5 years ago today?
P: DateFrom: 5 years ago today, DateTo: 5 years ago today, TuID: 1PQVARRA, Region: APAC

Q: I'm curious about the notional traded by RUUUJJQPVE Inc. with TuID AGLBKUTD on June 6th in APAC. Could you look that up?
P: DateFrom: June 6th at 8 AM, DateTo: June 6th at 8 AM, Tuid: AGLBKUTD, Region: APAC

Q: Could you provide the details on the notional value traded through execution point UFZJ in APAC region three months ago?
P: DateFrom: three months ago, DateTo: three months ago, ExecutionPoint: UFZJ

Q: Provide me the notional traded volume from 03-02-2024 to today in APAC region?
P: DateFrom: 03-02-2024, DateTo: today, Region: APAC

Q: Would you be able to tell me the total notional traded in EMEA for the client DYAIFGP Inc. with StarID on February 20th, 2024?
P: DateFrom: 2024-02-20, DateTo: 2024-02-20, Region: EMEA, StarID: 11207379

Q: Give me the notional volume traded from 13th may 2024 in the americas region?
P: DateFrom: 2024-05-13, Region: AMERICAS

Q: Give me the notional volume traded until 13th may 2024 in the americas region?
P: DateFrom: 2024-05-13, Region: AMERICAS

Q: Can you tell me how much notional was traded yesterday for single shares in the AMERICAS?
P: DateFrom: yesterday, DateTo: yesterday, Region: AMERICAS, BusinessLine: SINGLE_SHARES

Q: I'd like to know the total notional traded in EMEA for the gset business line 1 week ago.
P: DateFrom: 1 week ago, DateTo: 1 week ago, Region: EMEA, BusinessLine: GSET

Q: Could you provide the total notional traded for GSET in the AMERICAS region three days ago?
P: DateFrom: 3 days ago, DateTo: 3 days ago, Region: AMERICAS, BusinessLine: GSET

Q: What's the notional trading value from yesterday 5 years ago for all business lines in the AMERICAS?
P: DateFrom: yesterday 5 years ago, DateTo: yesterday 5 years ago, Region: AMERICAS

Q: What's the notional trading value from yesterday 1 week ago for all business lines in the AMERICAS?
P: DateFrom: yesterday 1 week ago, DateTo: yesterday 1 week ago, Region: AMERICAS

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
To be handled:

Q: What was the total notional traded last month in americas on NASD by BTIG?
P: DateFrom: 2024-04-01, DateTo: 2024-04-30, ExecutionPoint: NASD, Tuid: BTIG, Region: americas

- \b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?) (?:[1-3]?[0-9](?:st|nd|rd|th))\b
- \b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?) (?:[1-3]?[0-9](?:st|nd|rd|th)), [1-2][0-9]{3}\b
- \b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?) (?:[1-3]?[0-9](?:st|nd|rd|th)) [1-2][0-9]{3}\b
- \b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?) (?:[1-3]?[0-9])\b
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

BusinessLine:

- I need assistance with this particular business line
- Can you adjust the settings to this business line for me?
- Please consider this business line for my support request
- set to businessline 
- businessline = 
- BusinessLine:
- businessline : 
- This is the business line I am inquiring about
- For my query, use this as the business line
- The business line I need help with is this one
- My question relates to this business line.
- I’d like to proceed with this business line for my services
- the business line is 
- update business line to
- Use [] as the specified value 
- new business line is 

ExecutionPoint: 

- Please set the exec_point to WLDS
- I want to use WFAQ as the exec_point.
- Can we use UFZJ for the exec_point?
- Set the execution point parameter to WSPB
- Change the exec_point to QSTD
- Configure the exec_point as WLDS.
- I'd like the exec_point to be WFAQ
- Please update the exec_point to QSTD
- The exec_point should be set to WSPB now
- Switch the exec_point to QSTD,
- Can you change the exec_point to WSPB
- WFAQ should be our exec_point now
- execution point is
- execution point = 
- update the execuiton point to 
- new execution point is 

PrimeID:

- I want to use the primeid 11268647 for the client
- The primeid 488256 is what I intend to use
- I will be using the primeid 11207379
- Use the primeid 11281372 for the client
- Use the primeid 11281372
- the primeid 11236780 is the one I choose to use
- I choose to use the primeid 11268647
- The primeid 488256 is selected for usage.
- Utilize the primeid 11207379 for this operation
- I will be working with the primeid 11281372
- Update the primeid with the value 11281372
- Let's go with 11207379 for the parameter
- Use 11207379 as the specified value.
- Set 11281372 as the new value for the parameter
- The value 11207379 should be used here
- new primeid is 

StarID:

- Set the starid to 67890 
- use starid 99000 for this request
- Use starid 55566 for pulling up the information
- I need details for starid 99900
- Here's the starid I need: 55667
- I'd like to continue with starid 11223
- I’d prefer if you could fetch the details using starid 43210
- update starid to 
- starid which i want to use now is 
- new starid is 
- switch to using starid 11122

Tuid:

- Utilize TuID 5ZGM
- Implement features using TuID ON6HO
- Set the configuration to include TuID ON6HO
- Set the parameter to the value 5ZGM
- For this setting, please use ON6HO
- The correct value for the parameter is A9T5M
- Please change the parameter to AGLBKUTD
- I have decided to use 1PQVARRA for the parameter
- Input AGLBKUTD for the parameter value of Tuid
- update the parameter tuid to 5ZGM
- Make sure the parameter is set to 5ZGM
- Let's go with ON6HO for the parameter. 
- The value A9T5M is what we'll use
- Adjust the parameter to AGLBKUTD
- Use ON6HO as the specified value
- Update tuid to 

Region:

- I'd like to select the US Region
- set region to US
- using region us
- want to use region emea
- region = emea
- region: emea
- region is ASIA
- Set my region to APAC
- I am in the US, so let's go with that region
- I choose APAC as my region.
- update by region to us
- Make sure my region is set to ASIA
- set my region to apac
- I’m in the US, so please use that as my region
- configure my location to emea

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

"Can you clear all the current slots for me?"
"Please reset all the slots."
"I'd like to start over. Could you reset the slots?"
"Can you empty all the slots, please?"
"I need all the slots to be cleared. Thanks!"
"Please wipe all the slots clean."
"Can you remove all the values from the slots?"
"I want to reset everything. Can you clear the slots?"
"Clear out all the slots, please."
"Can you reset all the slots to their default state?"
"Please empty all the current slots."
"I'd like to clear all the slots now."
"Can you reset all the slots for me?"
"Please clear all the information in the slots."
"Can you make sure all the slots are reset?"
"I need to start fresh. Can you clear all the slots?"
"Reset all the slots, please."
"Could you empty out all the slots?"
"I'd like to have all the slots reset, thanks."
"Can you clear all the values from the slots for me?"
"Could you please clear all the current slot values?"
"I'd appreciate it if you could reset all the slots."
"Can you erase all the slots for me?"
"Please remove everything from the slots."
"I'd like to have all the slots cleared out."
"Can you reset every slot to empty?"
"Please reset all slots back to zero."
"Can you make sure all slot values are cleared?"
"I'd like to start again. Can you clear all slots?"
"Clear all slot data, please."
"Can you reset all slots and start fresh?"
"Please delete all the values in the slots."
"Can you make all the slots empty again?"
"I'd like to reset all the slots, can you do that?"
"Please clear out the current slots."
"Can you reset all the slots for a new start?"
"Please remove all current slot information."
"I'd like all slots to be reset, thanks."
"Can you clear all slot entries?"
"Please reset all slots to their initial state."

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

RASA_SERVER_URL = "http://localhost:5005/webhooks/rest/webhook"

@app.route('/chat', methods=['POST'])
def chat():
    user_message = request.json.get("message")
    sender_id = request.json.get("sender_id", "default")

    if not user_message:
        return jsonify({"error": "No message provided"}), 400

    response = requests.post(
        RASA_SERVER_URL,
        json={"sender": sender_id, "message": user_message}
    )

    if response.status_code != 200:
        return jsonify({"error": "Failed to get response from Rasa"}), 500

    return jsonify(response.json())

if __name__ == '__main__':
    app.run(port=8000, debug=True)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

import requests
import json

# Define the API endpoint
api_url = "http://localhost:8000/chat"

# Function to send a message to the chatbot and get the response
def send_message(message, sender_id="user1"):
    payload = {
        "message": message,
        "sender_id": sender_id
    }
    
    response = requests.post(api_url, json=payload)
    
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Failed to get response from API. Status code: {response.status_code}")
        print("Response:", response.text)
        return None

# Main loop to take user input and get chatbot responses
while True:
    user_message = input("You: ")
    
    if user_message.lower() in ["exit", "quit", "stop"]:
        print("Ending chat session.")
        break
    
    chat_response = send_message(user_message)
    
    if chat_response:
        for message in chat_response:
            print(f"Bot: {message['text']}")

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the pre-trained model and tokenizer
model_name = "meta-llama/LLaMA-7B"  # Replace with the specific model you want to use
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Prepare the input text
input_text = "Hello, how are you?"
inputs = tokenizer(input_text, return_tensors="pt")

# Generate a response
outputs = model.generate(inputs.input_ids, max_length=50)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(response)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Training data for intents and entity extraction is crucial for the performance of a Rasa NLU model.

Intents:

Intents represent the purpose or goal of a user's input. We need to define clear and distinct intents that cover all possible user inputs relevant to our application.

Entities:

Entities are structured pieces of information in user input that are relevant to the intents.


Points to be considered while Creating training data:

* While giving training examples for an intent try to cover all the diverse sets of examples of how a user might address that intent, it is advised that we reserve some keywords to an intent and when that keyword is given as input it becomes easy for the model to identify the intent of the input sentence.
* Try to balance the number of examples across intents to avoid biasing the model towards more frequent intents.
* Include diverse examples showing different formats and contexts in which an entity can appear to improve the model's ability to recognise entities under varied circumstances.
* If certain words or phrases mean the same thing in the context of your application, provide synonyms to help the model understand this. This increases the accuracy of entity extraction by mapping diverse inputs to a standard format.
* Use techniques like paraphrasing or artificially generating training data to increase the volume and diversity of your dataset, especially when you have limited examples this is known as Data Augmentation.
* But usage of more Synthetic data should be removed. 
* If you are interested in doing the entity extraction then try to give all the diverse sets of the entity values.
    * Ex:
    * For Tuid:
        * There are two versions of tuids which we need to focus on:
            * Alphanumeric
            * Alphabetical
        * Now we need to first sort the tuids as per the number of chars in the tuids and then for each total character we need to ensure that we have examples included that have both the two versions of tuids.
    * For Business Line:
    * For Date:
    * For StarID:
    * For PrimeID:
    * For Client Name:
    * For Execution Point:
	
Reference:
1. https://rasa.com/docs/rasa/training-data-format
2. https://rasa.com/docs/rasa/nlu-training-data

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

import pandas as pd

# Load the CSV file, assuming no header and the column name is 'tuid'
df = pd.read_csv('tuid.csv', header=None, names=['tuid'])

# Remove duplicates and blanks
df = df.drop_duplicates().dropna()
df['tuid'] = df['tuid'].str.strip()
df = df[df['tuid'] != '']

# Define a function to categorize each entry
def categorize_entry(entry):
    if entry.isalpha():
        return 'alphabetic'
    elif entry.isalnum():
        return 'alphanumeric'
    else:
        return 'other'

# Apply the categorization
df['category'] = df['tuid'].apply(categorize_entry)

# Sort by the length of the tuid, then by category, and then alphabetically within the same number of characters
df = df.sort_values(by=['tuid'], key=lambda x: (x.str.len(), x.map(categorize_entry), x))

# Save the processed data back to the same CSV file
df.to_csv('tuid.csv', index=False, header=False)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

import pandas as pd

# Load the CSV file, assuming no header and the column name temporarily set to 'primeid'
df = pd.read_csv('primeid.csv', header=None, names=['primeid'])

# Remove duplicates and blanks
df = df.drop_duplicates().dropna()
df['primeid'] = df['primeid'].str.strip()
df = df[df['primeid'] != '']

# Convert the entries to integers for sorting
df['primeid'] = pd.to_numeric(df['primeid'], errors='coerce')
df = df.dropna()  # Drop any rows that couldn't be converted to numbers

# Sort the DataFrame by the numerical value of 'primeid'
df = df.sort_values(by='primeid')

# Save the processed data back to the same CSV file
df.to_csv('primeid.csv', index=False, header=False)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Structuring Training data for Rasa


A very well-developed training data for intents and entity extraction is crucial for the performance of a Rasa NLU model.

Intents:

Intents represent the purpose or goal of a user's input. We need to define clear and distinct intents that cover all possible user inputs relevant to our application.

Entities:

Entities are structured pieces of information in user input that are relevant to the intents.


Points to be considered while Creating training data:
Preparing a training dataset for a natural language processing tasks such as entity recognition, involves creating a comprehensive collection of examples that can teach the model to perform accurately and robustly. A high-quality training dataset is critical because it directly influences the model's ability to generalize well to new, unseen data. The following are the key points that we need to lookup at when creating a new dataset.

Coverage and Diversity:
The main goal in dataset preparation is to ensure broad coverage and diversity. This means including examples from as many potential variations of the data as possible.
Ex: If the task involves recognizing named entities in text, the dataset should contain:
1. Synonyms text should be included with the same context and even the professional jargon should be included in the training dataset
2. Data in various formats

Balanced Classes:
It is important to maintain balance among different classes in the dataset. If some classes are underrepresented, the model may develop biases towards the more frequently represented classes, reducing its effectiveness on rarer but potentially important cases. 

Quality of Data:
The quality of data in the training set must be high, which means:
* Accurate, with minimal errors in labeling or categorization
* Relevant, containing examples pertinent to the specific tasks the model is expected to perform.
* Clean, with preprocessing steps taken to remove noise such as irrelevant information, formatting issues, or incorrect labels.

Contextual Richness:
For many tasks, the context in which data appears is very important. The dataset should include examples where entities and their identifiers appear in varied contextual settings to help the model learn the nuances of language usage and meaning i.e., we should look into how to ask the same question in many different formats.

Realistic and Challenging Examples:
Include challenging scenarios that the model might face after deployment, like:
* Edge cases or rare occurrences
* Ambiguous or confusing examples that require deep understanding or reasoning
* Examples which are specially defined for testing the models robustness. (If this example is not performed well by the model then add this to the training dataset. 

Continuous Updates:
Maintaining and updating the dataset periodically is essential as new types of data emerge for the domain, so continuous updates help ensure that the model remains effective over time.
* Reviewing regularly and expanding the dataset with new data is very important task of the model. 
* This can be achieved using including user feedback and error analysis findings to refine and improve the dataset.

Summarising for our use-case:
* While giving training examples for an intent try to cover all the diverse sets of examples of how a user might address that intent, it is advised that we reserve some keywords to an intent and when that keyword is given as input it becomes easy for the model to identify the intent of the input sentence.
* Try to balance the number of examples across intents to avoid biasing the model towards more frequent intents.
* Including diverse examples showing different formats and contexts in which an entity can appear to improve the model's ability to recognise entities under varied circumstances.
* If certain words or phrases mean the same thing in the context of your application, provide synonyms to help the model understand this. This increases the accuracy of entity extraction by mapping diverse inputs to a standard format.
* Using techniques like paraphrasing or artificially generating training data helps in increasing the volume and diversity of your dataset, especially when you have limited examples this is known as Data Augmentation.
* However, usage of more Synthetic data is not recommended.
* If you are interested in doing the entity extraction then try to give all the diverse sets of the entity values.

Cleaning the Data for training:
Data cleaning is an important step in data preparation process, which is primarily focused on identifying and correcting inaccuracies, inconsistencies, and errors in a dataset.
Key aspects:
* Removing Noise and Errors
* Handling Missing Data
* Identifying and Removing Outliers
* Ensuring Consistency
* Resolving Duplicate Data

For our use case, we are more interested in removing the duplicate data,  but we are not much focussed on removing the outliers and handling the error and noise, as for the chatbot some out-of-order data is not noise as it should be even able to identify the noise as well. 

Data cleaning is more prevalent for the entities tuid, execution point, starid, primeid but not for business line because as we have only defined set of elements in business line which is not dynamically updated often.


Tuid, Execution Point: We first categorise the data based on the number of characters and then we categorise if the datapoint is alphanumeric or alphabetical
Starid, primeid: Sorting of the datapoints is done.
Client Name: Sorted alphabetically.
See the attached files for codes for cleaning the data. 


Categorising the entities for detailed information for creating the dataset

    1. For Tuid:
        * There are two versions of tuids which we need to focus on:
            * Alphanumeric
            * Alphabetical
        * Now we need to first sort the tuids as per the number of chars in the tuids and then for each total character we need to ensure that we have examples included that have both the two versions of tuids.

    2. For Execution Point:
        * There can be two versions of execution points which we need to focus on:
            * Alphanumeric
            * Alphabetical
        * Now we need to first sort the exec points as per the number of chars in the exec points and then for each total character we need to ensure that we have examples included that have both the two versions of exec points.

    3. For Business Line:
        * There are only a handful of business lines so it is sufficient that we include all of those in the training examples

    4. For Date:
		We have a file for the dates where we have a comprehensive list of all the relative/ explicit dates people use in general conversations. 

    5. For StarID:
        * The starid’s are generally of the format 11000000’s but in some cases it can be 1/2/3/4 digit numbers also so choose the examples for training the model accordingly. 

    6. For PrimeID:
        * The primeid’s are generally of format 300000000’s and 1000000000’s without any anomalies [according to the csv given to me.

    7. For Client Name:
		This can be a tricky thing while preparing the training data because we cannot directly copy paste the datapoints in client_name.csv as people generally tend to use shortcuts of the client names like for ex., if we have Barclays Bank plc - London this is generally addressed as Barclays London. So while creating the dataset we should be careful that we have included even this method of representing a client name.
	
	8. For Region:
		There are only a handful of regions so it is sufficient that we include all of those in the training examples

	
Reference:
1. https://rasa.com/docs/rasa/training-data-format
2. https://rasa.com/docs/rasa/nlu-training-data
3. https://deepchecks.com/preparing-your-data-for-machine-learning-full-guide/
4. https://nix-united.com/blog/how-to-properly-prepare-your-dataset-for-machine-learning/
5. https://www.pecan.ai/blog/data-preparation-for-machine-learning/
























