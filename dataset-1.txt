--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Structuring Training data for Rasa


A very well-developed training data for intents and entity extraction is crucial for the performance of a Rasa NLU model.

Intents:

Intents represent the purpose or goal of a user's input. We need to define clear and distinct intents that cover all possible user inputs relevant to our application.

Entities:

Entities are structured pieces of information in user input that are relevant to the intents.


Points to be considered while Creating training data:
Preparing a training dataset for a natural language processing tasks such as entity recognition, involves creating a comprehensive collection of examples that can teach the model to perform accurately and robustly. A high-quality training dataset is critical because it directly influences the model's ability to generalize well to new, unseen data. The following are the key points that we need to lookup at when creating a new dataset.

Coverage and Diversity:
The main goal in dataset preparation is to ensure broad coverage and diversity. This means including examples from as many potential variations of the data as possible.
Ex: If the task involves recognizing named entities in text, the dataset should contain:
1. Synonyms text should be included with the same context and even the professional jargon should be included in the training dataset
2. Data in various formats

Balanced Classes:
It is important to maintain balance among different classes in the dataset. If some classes are underrepresented, the model may develop biases towards the more frequently represented classes, reducing its effectiveness on rarer but potentially important cases. 

Quality of Data:
The quality of data in the training set must be high, which means:
* Accurate, with minimal errors in labeling or categorization
* Relevant, containing examples pertinent to the specific tasks the model is expected to perform.
* Clean, with preprocessing steps taken to remove noise such as irrelevant information, formatting issues, or incorrect labels.

Contextual Richness:
For many tasks, the context in which data appears is very important. The dataset should include examples where entities and their identifiers appear in varied contextual settings to help the model learn the nuances of language usage and meaning i.e., we should look into how to ask the same question in many different formats.

Realistic and Challenging Examples:
Include challenging scenarios that the model might face after deployment, like:
* Edge cases or rare occurrences
* Ambiguous or confusing examples that require deep understanding or reasoning
* Examples which are specially defined for testing the models robustness. (If this example is not performed well by the model then add this to the training dataset. 

Continuous Updates:
Maintaining and updating the dataset periodically is essential as new types of data emerge for the domain, so continuous updates help ensure that the model remains effective over time.
* Reviewing regularly and expanding the dataset with new data is very important task of the model. 
* This can be achieved using including user feedback and error analysis findings to refine and improve the dataset.

Summarising for our use-case:
* While giving training examples for an intent try to cover all the diverse sets of examples of how a user might address that intent, it is advised that we reserve some keywords to an intent and when that keyword is given as input it becomes easy for the model to identify the intent of the input sentence.
* Try to balance the number of examples across intents to avoid biasing the model towards more frequent intents.
* Including diverse examples showing different formats and contexts in which an entity can appear to improve the model's ability to recognise entities under varied circumstances.
* If certain words or phrases mean the same thing in the context of your application, provide synonyms to help the model understand this. This increases the accuracy of entity extraction by mapping diverse inputs to a standard format.
* Using techniques like paraphrasing or artificially generating training data helps in increasing the volume and diversity of your dataset, especially when you have limited examples this is known as Data Augmentation.
* However, usage of more Synthetic data is not recommended.
* If you are interested in doing the entity extraction then try to give all the diverse sets of the entity values.

Cleaning the Data for training:
Data cleaning is an important step in data preparation process, which is primarily focused on identifying and correcting inaccuracies, inconsistencies, and errors in a dataset.
Key aspects:
* Removing Noise and Errors
* Handling Missing Data
* Identifying and Removing Outliers
* Ensuring Consistency
* Resolving Duplicate Data

For our use case, we are more interested in removing the duplicate data,  but we are not much focussed on removing the outliers and handling the error and noise, as for the chatbot some out-of-order data is not noise as it should be even able to identify the noise as well. 

Data cleaning is more prevalent for the entities tuid, execution point, starid, primeid but not for business line because as we have only defined set of elements in business line which is not dynamically updated often.


Tuid, Execution Point: We first categorise the data based on the number of characters and then we categorise if the datapoint is alphanumeric or alphabetical
Starid, primeid: Sorting of the datapoints is done.
Client Name: Sorted alphabetically.
See the attached files for codes for cleaning the data. 


Categorising the entities for detailed information for creating the dataset

    1. For Tuid:
        * There are two versions of tuids which we need to focus on:
            * Alphanumeric
            * Alphabetical
        * Now we need to first sort the tuids as per the number of chars in the tuids and then for each total character we need to ensure that we have examples included that have both the two versions of tuids.

    2. For Execution Point:
        * There can be two versions of execution points which we need to focus on:
            * Alphanumeric
            * Alphabetical
        * Now we need to first sort the exec points as per the number of chars in the exec points and then for each total character we need to ensure that we have examples included that have both the two versions of exec points.

    3. For Business Line:
        * There are only a handful of business lines so it is sufficient that we include all of those in the training examples

    4. For Date:
		We have a file for the dates where we have a comprehensive list of all the relative/ explicit dates people use in general conversations. 

    5. For StarID:
        * The starid’s are generally of the format 11000000’s but in some cases it can be 1/2/3/4 digit numbers also so choose the examples for training the model accordingly. 

    6. For PrimeID:
        * The primeid’s are generally of format 300000000’s and 1000000000’s without any anomalies [according to the csv given to me.

    7. For Client Name:
		This can be a tricky thing while preparing the training data because we cannot directly copy paste the datapoints in client_name.csv as people generally tend to use shortcuts of the client names like for ex., if we have Barclays Bank plc - London this is generally addressed as Barclays London. So while creating the dataset we should be careful that we have included even this method of representing a client name.
	
	8. For Region:
		There are only a handful of regions so it is sufficient that we include all of those in the training examples

	
Reference:
1. https://rasa.com/docs/rasa/training-data-format
2. https://rasa.com/docs/rasa/nlu-training-data
3. https://deepchecks.com/preparing-your-data-for-machine-learning-full-guide/
4. https://nix-united.com/blog/how-to-properly-prepare-your-dataset-for-machine-learning/
5. https://www.pecan.ai/blog/data-preparation-for-machine-learning/

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
import re

# Define the regex pattern
pattern = r'\b(client|clientname|name)\b\s+([A-Za-z0-9\s]+(?:,\s+[A-Za-z0-9\s]+)*)'

# Compile the pattern
regex = re.compile(pattern)

# Example strings
texts = [
    "client    apple LLC,     meta ai, alphabet today",
    "clientname    apple123 LLC",
    "name    meta ai, alphabet yesterday",
    "client 12345, abc123"
]

# Function to check if a name is alphanumeric and not purely numeric
def is_valid_name(name):
    return any(c.isalpha() for c in name) and any(c.isalnum() for c in name)

# Extract and split company names
for text in texts:
    match = regex.search(text)
    if match:
        company_names = match.group(2)
        company_list = [name.strip() for name in company_names.split(',') if is_valid_name(name.strip())]
        print(f"Company names: {company_list}")
    else:
        print("No match found")

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

import spacy

# Load the spaCy model
nlp = spacy.load("en_core_web_trf")

def extract_date_entities(text):
    # Process the text
    doc = nlp(text)
    # Extract entities identified as dates
    dates = [ent.text for ent in doc.ents if ent.label_ == "DATE"]
    return dates

# Example usage
input_text = "I have an appointment on June 25th, and another important meeting on July 3rd."
dates = extract_date_entities(input_text)
print("Extracted Date Entities:", dates)

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------








